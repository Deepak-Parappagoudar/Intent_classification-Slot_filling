{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GnqzJR_vzSXY"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "qa=pd.read_csv('qa.csv')\n",
    "\n",
    "df = pd.read_csv('final_train.csv')\n",
    "data=df[(df['intent']=='action' ) | (df['intent']=='query' ) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6sEuo0CLuGfK",
    "outputId": "b1f3f481-3bb1-47f4-e0c1-c23f045554e7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>API</th>\n",
       "      <th>function</th>\n",
       "      <th>description</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca.randomfox</td>\n",
       "      <td>floof</td>\n",
       "      <td>random fox picture</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>com.abcnews</td>\n",
       "      <td>top_stories</td>\n",
       "      <td>get top stories on ABC News</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>com.appspot.randomuselessfact</td>\n",
       "      <td>random</td>\n",
       "      <td>random useless fact</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>com.arstechnica</td>\n",
       "      <td>index</td>\n",
       "      <td>get ars technica index</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>com.bbc</td>\n",
       "      <td>top_stories</td>\n",
       "      <td>get top stories on BBC News</td>\n",
       "      <td>query</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             API     function                  description  \\\n",
       "0                   ca.randomfox        floof           random fox picture   \n",
       "1                    com.abcnews  top_stories  get top stories on ABC News   \n",
       "2  com.appspot.randomuselessfact       random          random useless fact   \n",
       "3                com.arstechnica        index       get ars technica index   \n",
       "4                        com.bbc  top_stories  get top stories on BBC News   \n",
       "\n",
       "  intent  \n",
       "0  query  \n",
       "1  query  \n",
       "2  query  \n",
       "3  query  \n",
       "4  query  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = qa.drop(qa.columns[0], axis=1)\n",
    "qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VuPhYq8n5eUP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepak\\AppData\\Local\\Temp\\ipykernel_17060\\1908698413.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text']= data['text'].apply(lambda x: removepunt(x))\n",
      "C:\\Users\\Deepak\\AppData\\Local\\Temp\\ipykernel_17060\\1908698413.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text']=data.text.apply(lambda x: remove_URL(x))\n",
      "C:\\Users\\Deepak\\AppData\\Local\\Temp\\ipykernel_17060\\1908698413.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['text']=data['text'].apply(lambda x: remove_art(x))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string,re\n",
    "string.punctuation\n",
    "def removepunt(text):\n",
    "  puntfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "  return puntfree\n",
    "data['text']= data['text'].apply(lambda x: removepunt(x))\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "data['text']=data.text.apply(lambda x: remove_URL(x))\n",
    "\n",
    "## stop words##\n",
    "# from nltk.corpus import stopwords\n",
    "# stop=set(stopwords.words('english'))\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#   text=[word.lower() for word in text.split() if word not in stop]\n",
    "\n",
    "#   return \" \".join(text)\n",
    "\n",
    "# data['text']=data.text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "##remove articles \n",
    "\n",
    "articles=['a','an','the']\n",
    "def remove_art(text):\n",
    "    text=[word.lower() for word in text.split() if word not in articles]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "data['text']=data['text'].apply(lambda x: remove_art(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.3.1-cp39-cp39-win_amd64.whl (11.7 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.8-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp39-cp39-win_amd64.whl (448 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\deepak\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 5.1.0\n",
      "    Uninstalling smart-open-5.1.0:\n",
      "      Successfully uninstalled smart-open-5.1.0\n",
      "Successfully installed blis-0.7.8 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.2 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.1 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.17 typer-0.4.2 wasabi-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FT3qovnb8U81",
    "outputId": "a8526e4d-4455-4543-fcc6-0b89e7ab45b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.cli.download(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LHU4TD_78U6W"
   },
   "outputs": [],
   "source": [
    "#vectorize tokens\n",
    "corpus=[nlp(row) for row in data['text'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9GOG9xszW0cu"
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "gb70pU6P8U32",
    "outputId": "64128893-44ea-49c1-fcac-1fb2d60cc0e7"
   },
   "outputs": [],
   "source": [
    "scores=[]\n",
    "text_id=[]\n",
    "ref_id=[]\n",
    "for i in range(len(data['text'])):\n",
    "    for q in range(len(qa['description'])):\n",
    "        if data['intent'][i]==qa['intent'][q]:\n",
    "        \n",
    "            ref_sent=qa['description'][q]\n",
    "            ref_sent_vec=nlp(ref_sent)\n",
    "\n",
    "            score=corpus[i].similarity(ref_sent_vec)\n",
    "            scores.append(score)\n",
    "            text_id.append(i)\n",
    "            ref_id.append(q)\n",
    "scores_doc=pd.DataFrame(list(zip(text_id,ref_id, scores)), columns=['text_id','ref_id','scores'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yU9A1O9hydHg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JotosWNnZK6s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "slot_glove.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
